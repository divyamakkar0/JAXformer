{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b579430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Github URL where python scripts are stored.\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/uvadlc_notebooks/master/docs/tutorial_notebooks/scaling/JAX/\"\n",
    "# Files to download.\n",
    "python_files = [\"single_gpu.py\", \"utils.py\"]\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in python_files:\n",
    "    if not os.path.isfile(file_name):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_name)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file directly from the GitHub repository, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af7eae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simulate_CPU_devices\n",
    "\n",
    "simulate_CPU_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69eae140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from pprint import pprint\n",
    "from typing import Any, Callable, Dict, Sequence, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from absl import logging\n",
    "from jax import lax\n",
    "from jax.experimental.shard_map import shard_map\n",
    "from jax.sharding import Mesh\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from ml_collections import ConfigDict\n",
    "\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b91d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MIT License.\n",
    "\n",
    "Copyright (c) 2024 Phillip Lippe\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\"\"\"\n",
    "from typing import Any, Callable, Dict, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax.struct import dataclass\n",
    "from flax.training import train_state\n",
    "\n",
    "# Type aliases\n",
    "PyTree = Any\n",
    "Metrics = Dict[str, Tuple[jax.Array, ...]]\n",
    "\n",
    "\n",
    "class TrainState(train_state.TrainState):\n",
    "    rng: jax.Array\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Batch:\n",
    "    inputs: jax.Array\n",
    "    labels: jax.Array\n",
    "\n",
    "\n",
    "def accumulate_gradients_loop(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    rng: jax.random.PRNGKey,\n",
    "    num_minibatches: int,\n",
    "    loss_fn: Callable,\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        batch: Full training batch.\n",
    "        rng: Random number generator to use.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "        loss_fn: Loss function to calculate gradients and metrics.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with accumulated gradients and metrics over the minibatches.\n",
    "    \"\"\"\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    minibatch_size = batch_size // num_minibatches\n",
    "    rngs = jax.random.split(rng, num_minibatches)\n",
    "    # Define gradient function for single minibatch.\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    # Prepare loop variables.\n",
    "    grads = None\n",
    "    metrics = None\n",
    "    for minibatch_idx in range(num_minibatches):\n",
    "        with jax.named_scope(f\"minibatch_{minibatch_idx}\"):\n",
    "            # Split the batch into minibatches.\n",
    "            start = minibatch_idx * minibatch_size\n",
    "            end = start + minibatch_size\n",
    "            minibatch = jax.tree.map(lambda x: x[start:end], batch)\n",
    "            # Calculate gradients and metrics for the minibatch.\n",
    "            (_, step_metrics), step_grads = grad_fn(\n",
    "                state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
    "            )\n",
    "            # Accumulate gradients and metrics across minibatches.\n",
    "            if grads is None:\n",
    "                grads = step_grads\n",
    "                metrics = step_metrics\n",
    "            else:\n",
    "                grads = jax.tree.map(jnp.add, grads, step_grads)\n",
    "                metrics = jax.tree.map(jnp.add, metrics, step_metrics)\n",
    "    # Average gradients over minibatches.\n",
    "    grads = jax.tree.map(lambda g: g / num_minibatches, grads)\n",
    "    return grads, metrics\n",
    "\n",
    "\n",
    "def accumulate_gradients_scan(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    rng: jax.random.PRNGKey,\n",
    "    num_minibatches: int,\n",
    "    loss_fn: Callable,\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
    "\n",
    "    In this version, we use `jax.lax.scan` to loop over the minibatches. This is more efficient in terms of compilation time.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        batch: Full training batch.\n",
    "        rng: Random number generator to use.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "        loss_fn: Loss function to calculate gradients and metrics.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with accumulated gradients and metrics over the minibatches.\n",
    "    \"\"\"\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    minibatch_size = batch_size // num_minibatches\n",
    "    rngs = jax.random.split(rng, num_minibatches)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    def _minibatch_step(minibatch_idx: jax.Array | int) -> Tuple[PyTree, Metrics]:\n",
    "        \"\"\"Determine gradients and metrics for a single minibatch.\"\"\"\n",
    "        minibatch = jax.tree.map(\n",
    "            lambda x: jax.lax.dynamic_slice_in_dim(  # Slicing with variable index (jax.Array).\n",
    "                x, start_index=minibatch_idx * minibatch_size, slice_size=minibatch_size, axis=0\n",
    "            ),\n",
    "            batch,\n",
    "        )\n",
    "        (_, step_metrics), step_grads = grad_fn(\n",
    "            state.params, state.apply_fn, minibatch, rngs[minibatch_idx]\n",
    "        )\n",
    "        return step_grads, step_metrics\n",
    "\n",
    "    def _scan_step(\n",
    "        carry: Tuple[PyTree, Metrics], minibatch_idx: jax.Array | int\n",
    "    ) -> Tuple[Tuple[PyTree, Metrics], None]:\n",
    "        \"\"\"Scan step function for looping over minibatches.\"\"\"\n",
    "        step_grads, step_metrics = _minibatch_step(minibatch_idx)\n",
    "        carry = jax.tree.map(jnp.add, carry, (step_grads, step_metrics))\n",
    "        return carry, None\n",
    "\n",
    "    # Determine initial shapes for gradients and metrics.\n",
    "    grads_shapes, metrics_shape = jax.eval_shape(_minibatch_step, 0)\n",
    "    grads = jax.tree.map(lambda x: jnp.zeros(x.shape, x.dtype), grads_shapes)\n",
    "    metrics = jax.tree.map(lambda x: jnp.zeros(x.shape, x.dtype), metrics_shape)\n",
    "    # Loop over minibatches to determine gradients and metrics.\n",
    "    (grads, metrics), _ = jax.lax.scan(\n",
    "        _scan_step, init=(grads, metrics), xs=jnp.arange(num_minibatches), length=num_minibatches\n",
    "    )\n",
    "    # Average gradients over minibatches.\n",
    "    grads = jax.tree.map(lambda g: g / num_minibatches, grads)\n",
    "    return grads, metrics\n",
    "\n",
    "\n",
    "def accumulate_gradients(\n",
    "    state: TrainState,\n",
    "    batch: Batch,\n",
    "    rng: jax.random.PRNGKey,\n",
    "    num_minibatches: int,\n",
    "    loss_fn: Callable,\n",
    "    use_scan: bool = False,\n",
    ") -> Tuple[PyTree, Metrics]:\n",
    "    \"\"\"Calculate gradients and metrics for a batch using gradient accumulation.\n",
    "\n",
    "    This function supports scanning over the minibatches using `jax.lax.scan` or using a for loop.\n",
    "\n",
    "    Args:\n",
    "        state: Current training state.\n",
    "        batch: Full training batch.\n",
    "        rng: Random number generator to use.\n",
    "        num_minibatches: Number of minibatches to split the batch into. Equal to the number of gradient accumulation steps.\n",
    "        loss_fn: Loss function to calculate gradients and metrics.\n",
    "        use_scan: Whether to use `jax.lax.scan` for looping over the minibatches.\n",
    "\n",
    "    Returns:\n",
    "        Tuple with accumulated gradients and metrics over the minibatches.\n",
    "    \"\"\"\n",
    "    if use_scan:\n",
    "        return accumulate_gradients_scan(\n",
    "            state=state, batch=batch, rng=rng, num_minibatches=num_minibatches, loss_fn=loss_fn\n",
    "        )\n",
    "    else:\n",
    "        return accumulate_gradients_loop(\n",
    "            state=state, batch=batch, rng=rng, num_minibatches=num_minibatches, loss_fn=loss_fn\n",
    "        )\n",
    "\n",
    "\n",
    "def print_metrics(metrics: Metrics, title: str | None = None) -> None:\n",
    "    \"\"\"Prints metrics with an optional title.\"\"\"\n",
    "    metrics = jax.device_get(metrics)\n",
    "    lines = [f\"{k}: {v[0] / v[1]:.6f}\" for k, v in metrics.items()]\n",
    "    if title:\n",
    "        title = f\" {title} \"\n",
    "        max_len = max(len(title), max(map(len, lines)))\n",
    "        lines = [title.center(max_len, \"=\")] + lines\n",
    "    print(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "def get_num_params(state: TrainState) -> int:\n",
    "    \"\"\"Calculate the number of parameters in the model.\"\"\"\n",
    "    return sum(np.prod(x.shape) for x in jax.tree_util.tree_leaves(state.params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c83e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fold_rng_over_axis(rng: jax.random.PRNGKey, axis_name: str) -> jax.random.PRNGKey:\n",
    "    \"\"\"Folds the random number generator over the given axis.\n",
    "\n",
    "    This is useful for generating a different random number for each device\n",
    "    across a certain axis (e.g. the model axis).\n",
    "\n",
    "    Args:\n",
    "        rng: The random number generator.\n",
    "        axis_name: The axis name to fold the random number generator over.\n",
    "\n",
    "    Returns:\n",
    "        A new random number generator, different for each device index along the axis.\n",
    "    \"\"\"\n",
    "    axis_index = jax.lax.axis_index(axis_name)\n",
    "    return jax.random.fold_in(rng, axis_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8687d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DPClassifier(nn.Module):\n",
    "    config: ConfigDict\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jax.Array, train: bool) -> jax.Array:\n",
    "        x = nn.Dense(\n",
    "            features=self.config.hidden_size,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"input_dense\",\n",
    "        )(x)\n",
    "        x = nn.silu(x)\n",
    "        x = nn.Dropout(rate=self.config.dropout_rate, deterministic=not train)(x)\n",
    "        x = nn.Dense(\n",
    "            features=self.config.num_classes,\n",
    "            dtype=self.config.dtype,\n",
    "            name=\"output_dense\",\n",
    "        )(x)\n",
    "        x = x.astype(jnp.float32)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f730793b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = ConfigDict(\n",
    "    dict(\n",
    "        batch_size=128,\n",
    "        num_classes=10,\n",
    "        input_size=784,\n",
    "    )\n",
    ")\n",
    "model_config = ConfigDict(\n",
    "    dict(\n",
    "        hidden_size=512,\n",
    "        dropout_rate=0.1,\n",
    "        dtype=jnp.bfloat16,\n",
    "        num_classes=data_config.num_classes,\n",
    "        data_axis_name=\"data\",\n",
    "    )\n",
    ")\n",
    "optimizer_config = ConfigDict(\n",
    "    dict(\n",
    "        learning_rate=1e-3,\n",
    "        num_minibatches=4,\n",
    "    )\n",
    ")\n",
    "config = ConfigDict(\n",
    "    dict(\n",
    "        model=model_config,\n",
    "        optimizer=optimizer_config,\n",
    "        data=data_config,\n",
    "        data_axis_name=model_config.data_axis_name,\n",
    "        seed=42,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8384ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dp = DPClassifier(config=config.model)\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=config.optimizer.learning_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "645661ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(config.seed)\n",
    "model_init_rng, data_inputs_rng, data_labels_rng = jax.random.split(rng, 3)\n",
    "batch = Batch(\n",
    "    inputs=jax.random.normal(data_inputs_rng, (config.data.batch_size, config.data.input_size)),\n",
    "    labels=jax.random.randint(\n",
    "        data_labels_rng, (config.data.batch_size,), 0, config.data.num_classes\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02404a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dp(rng: jax.random.PRNGKey, x: jax.Array, model: nn.Module) -> TrainState:\n",
    "    init_rng, rng = jax.random.split(rng)\n",
    "    variables = model.init({\"params\": init_rng}, x, train=False)\n",
    "    params = variables.pop(\"params\")\n",
    "    state = TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=params,\n",
    "        tx=optimizer,\n",
    "        rng=rng,\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aaa3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),\n",
       "       CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_array = np.array(jax.devices())\n",
    "mesh = Mesh(device_array, (config.data_axis_name,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd29e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_dp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        functools.partial(init_dp, model=model_dp),\n",
    "        mesh,\n",
    "        in_specs=(P(), P(config.data_axis_name)),\n",
    "        out_specs=P(),\n",
    "        check_rep=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "671d1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Parameters\n",
      "{'input_dense': {'bias': ((512,),\n",
      "                          NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "                 'kernel': ((784, 512),\n",
      "                            NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host))},\n",
      " 'output_dense': {'bias': ((10,),\n",
      "                           NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "                  'kernel': ((512, 10),\n",
      "                             NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host))}}\n"
     ]
    }
   ],
   "source": [
    "state_dp = init_dp_fn(model_init_rng, batch.inputs)\n",
    "print(\"DP Parameters\")\n",
    "pprint(jax.tree.map(lambda x: (x.shape, x.sharding), state_dp.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a7e9278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(\n",
    "    params: PyTree, apply_fn: Any, batch: Batch, rng: jax.Array\n",
    ") -> Tuple[jax.Array, Dict[str, Any]]:\n",
    "    # Since dropout masks vary across the batch dimension, we want each device to generate a\n",
    "    # different mask. We can achieve this by folding the rng over the data axis, so that each\n",
    "    # device gets a different rng and thus mask.\n",
    "    dropout_rng = fold_rng_over_axis(rng, config.data_axis_name)\n",
    "    # Remaining computation is the same as before for single device.\n",
    "    logits = apply_fn({\"params\": params}, batch.inputs, train=True, rngs={\"dropout\": dropout_rng})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch.labels)\n",
    "    correct_pred = jnp.equal(jnp.argmax(logits, axis=-1), batch.labels)\n",
    "    batch_size = batch.inputs.shape[0]\n",
    "    step_metrics = {\"loss\": (loss.sum(), batch_size), \"accuracy\": (correct_pred.sum(), batch_size)}\n",
    "    loss = loss.mean()\n",
    "    return loss, step_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a59908b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_dp(\n",
    "    state: TrainState,\n",
    "    metrics: Metrics | None,\n",
    "    batch: Batch,\n",
    ") -> Tuple[TrainState, Metrics]:\n",
    "    rng, step_rng = jax.random.split(state.rng)\n",
    "    grads, step_metrics = accumulate_gradients(\n",
    "        state,\n",
    "        batch,\n",
    "        step_rng,\n",
    "        config.optimizer.num_minibatches,\n",
    "        loss_fn=loss_fn,\n",
    "    )\n",
    "    # Update parameters. We need to sync the gradients across devices before updating.\n",
    "    with jax.named_scope(\"sync_gradients\"):\n",
    "        grads = jax.tree.map(lambda g: jax.lax.pmean(g, axis_name=config.data_axis_name), grads)\n",
    "    new_state = state.apply_gradients(grads=grads, rng=rng)\n",
    "    # Sum metrics across replicas. Alternatively, we could keep the metrics separate\n",
    "    # and only synchronize them before logging. For simplicity, we sum them here.\n",
    "    with jax.named_scope(\"sync_metrics\"):\n",
    "        step_metrics = jax.tree.map(\n",
    "            lambda x: jax.lax.psum(x, axis_name=config.data_axis_name), step_metrics\n",
    "        )\n",
    "    if metrics is None:\n",
    "        metrics = step_metrics\n",
    "    else:\n",
    "        metrics = jax.tree.map(jnp.add, metrics, step_metrics)\n",
    "    return new_state, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff3adfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_dp_fn = jax.jit(\n",
    "    shard_map(\n",
    "        train_step_dp,\n",
    "        mesh,\n",
    "        in_specs=(P(), P(), P(config.data_axis_name)),\n",
    "        out_specs=(P(), P()),\n",
    "        check_rep=False,\n",
    "    ),\n",
    "    donate_argnames=(\"state\", \"metrics\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb6b7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PjitFunction of <function train_step_dp at 0x11c454040>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_dp_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "342bd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, metric_shapes = jax.eval_shape(\n",
    "    train_step_dp_fn,\n",
    "    state_dp,\n",
    "    None,\n",
    "    batch,\n",
    ")\n",
    "metrics_dp = jax.tree.map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b8e96d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.000000\n",
      "loss: 0.002246\n"
     ]
    }
   ],
   "source": [
    "for _ in range(15):\n",
    "    state_dp, metrics_dp = train_step_dp_fn(state_dp, metrics_dp, batch)\n",
    "final_metrics_dp = jax.tree.map(lambda x: jnp.zeros(x.shape, dtype=x.dtype), metric_shapes)\n",
    "state_dp, final_metrics_dp = train_step_dp_fn(state_dp, final_metrics_dp, batch)\n",
    "print_metrics(final_metrics_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28588953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP Parameters\n",
      "{'input_dense': {'bias': ((512,),\n",
      "                          NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "                 'kernel': ((784, 512),\n",
      "                            NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host))},\n",
      " 'output_dense': {'bias': ((10,),\n",
      "                           NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "                  'kernel': ((512, 10),\n",
      "                             NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host))}}\n",
      "Metrics\n",
      "{'accuracy': (((),\n",
      "               NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "              ((),\n",
      "               NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host))),\n",
      " 'loss': (((),\n",
      "           NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)),\n",
      "          ((),\n",
      "           NamedSharding(mesh=Mesh('data': 8, axis_types=(Auto,)), spec=PartitionSpec(), memory_kind=unpinned_host)))}\n"
     ]
    }
   ],
   "source": [
    "print(\"DP Parameters\")\n",
    "pprint(jax.tree.map(lambda x: (x.shape, x.sharding), state_dp.params))\n",
    "print(\"Metrics\")\n",
    "pprint(jax.tree.map(lambda x: (x.shape, x.sharding), final_metrics_dp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
